\label{prelimBayesian}
\begin{DoubleSpace*}
Bayesian statistics represents both a sub-discipline of statistics as well as a philosophical paradigm. A philosophical discussion of the relative merits of frequentist versus Bayesian statistics is beyond the scope of this text. However, we do note the following general distinctions between frequentist and Bayesian statistics. First, frequentist approaches assume that a population parameter is fixed, while samples are drawn (a stochastic or random process) from a population to estimate this population parameter. Contrastingly, Bayesian approaches assume that the sample data is fixed, but the population parameter is a random variate for which a probability distribution can be specified. Given a fixed population parameter, frequentist approaches are based on the long run behavior of drawing repeated samples from a population. For example, a 95\% confidence interval (CI) for a population parameter means that if 100 samples were randomly drawn a CI constructed in an identical manner 95 would contain the population value on average. This implies that the frequentist approaches consider the likelihood of both observed and unobserved data. In contrast, Bayesian approaches evaluate the probability distribution of a population parameter the observed sample. This probability distribution can be determined by considering the integrating the likelihood of the  observed sample given each possible parameter values times the probability of these parameter values, divided by the likelihood of the sample. Consequently, Bayesian approaches require the prior stipulation of the probability distribution of the population parameter.
 
\section{Bayes rule}
The central foundation of Bayesian statistics is Bayes rule \cite{gelman2004}. 
\begin{theorem} If $A$ and $B$ are events with $\PP(B)>0$, then:
	\begin{gather}
		\PP(A|B) = \frac{\PP(B|A)\PP(A)}{\PP(B)}
	\end{gather}.	
\end{theorem}

Further, Bayes rule can be extended to any arbitrary partitioning of a sample space \cite{casella2002}.
\begin{theorem}
	Let $\mathcal{P}=\{A_i: i=1,2, \hdots, N\}$ represents an arbitrary partitioning of a sample space $S$, then for a specific $A_i$:
	\begin{gather}
			 \PP(A_i|B)=\frac{\PP(B|A_i)P(A_i)}{\sum_{A_j\in \mathcal{P}}\PP(B|A_j)P(A_j)}.
	\end{gather}
\end{theorem}

\section{Bayesian inference regarding a parameter}
Let $\theta$ (or $\boldsymbol{\theta}$ in the multivariate case) represent a parameter of a probability mass function (pmf) or probability distribution function (pdf)  $f(x|\theta)$ of a random variable $X$. Without loss of generality, we discuss the univariate case of one random variable $X$ and one population parameter $\theta$, although multivariate generalization is straightforward. The objective of statistical inference regarding $\theta$ is to utilize a random sample $X_1, X_2, \hdots X_n$ from a population with pmf/pdf $f(x|\theta)$ to determine likely values of $\theta$ \cite{casella2002}. Denoting a fixed sample as $\textbf{x}=\{x_1, x_2,\hdots, x_n \}$, we note that $\textbf{x}$ is a realization from the sample space $\mathcal{X}$ of all possible samples from the population. In Bayesian inference, the population parameter $\theta$ is regarded as unknown \cite{hoff2009}. It is assumed that the uncertainty regarding the true population value of $\theta$, can be represented by the prior probability distribution $p(\theta)$. Since the true population value is unknown, a set of possible values for $\theta$, that is the parameter space $\Theta$ comprises the support of $p(\theta)$. In Bayesian analysis, a sampling model describing the probability of observing realization $x$ of the random variable $X$ conditioned on a fixed value of the parameter $\theta$, that is $p(x|\theta)$ must also be specified. When considering a fixed sample, this term is also referred to as the  likelihood \cite{casella2002}. 
\begin{theorem}
	 If $X_1, X_2, \hdots, X_n$ are independent and identically distributed (iid) random variables, then $f(x_1,x_2,\hdots, x_n|\theta)=\prod_{i=1}^n f(x_i|\theta)$.
\end{theorem}
More specifically, we refer to the joint distribution function of a sample $x$, conditional on the value of the parameter $\theta$ as the likelihood of theta given the sample, or $\mathcal{L}(\theta|\textbf{x})$. Given a prior distribution for $\theta\in \Theta$, a random sample from the population $\textbf{x}$,and a sampling model or likelihood, the posterior distribution for $\theta$ can then be determined utilizing Bayes rule, as below\cite{hoff2009}:
\begin{gather}
	p(\theta|\textbf{x})=\frac{p(\textbf{x}|\theta)p(\theta)}{\int_{\tilde{\theta} \in \Theta} p(\textbf{x}|\tilde{\theta})p(\tilde{\theta})d\tilde{\theta}}=\frac{p(\textbf{x}|\theta)p(\theta)}{p(\textbf{x})}.
\end{gather}
The denominator term is referred to as the marginal likelihood. As the marginal likelihood involves only a fixed sample, the denominator is a constant, which justifies the following relation involving the posterior distribution of $\theta$:
\begin{gather}
	p(\theta|\textbf{x}) \propto p(\textbf{x}|\theta)p(\theta). 
\end{gather}
From this relation, it can be noted that the posterior distribution for $\theta$, after observing a random sample $\textbf{x}$ is proportional to the likelihood times the prior.

\section{Example Bayesian inference regarding a parameter}
Consider a mass spectrometer with a counting detector that records the counts per second over a fixed $m/z$ window. The number of counts recorded by the detector over a fixed time interval may be a Poisson process. Let $X$ be the random variable representing the number of counts recorded by the detector over a one second time interval, with an average number of counts per second (population parameter) of $\lambda$. We then say that $X\sim Pois(\lambda)$, and note that the probability mass function for $X$ is:
\begin{gather}
	f(x|\lambda)=\frac{\lambda^x e^{-\lambda}}{x!}, \quad x \in \mathbb{N}, \quad \lambda \in \left[0, \infty \right)
\end{gather}
 Assume a sample has been observed with the following counts: $\{107, 103, 110, 99, 108, 108, 105\}$ and that we wish to determine plausible values of the rate parameter $\lambda$. In order to determine plausible values, we will conduct Bayesian inference over $\lambda$. Assume that we have collected ten prior count observations and that the sum of counts from these observations was 1,000. We may then choose a prior distribution for $\lambda$ of $Gamma(\alpha, \beta)$ with parameters $\alpha=1000$ and $\beta=10$. We note the following form of the Gamma distribution pdf: 
\begin{gather}
	f(\lambda|\alpha,\beta) =\frac{\beta^{\alpha}}{\Gamma(\alpha)}\lambda^{\alpha-1}e^{-\beta \lambda}, \quad \lambda \in \left[0, \infty \right), \quad \alpha,\beta >0.
\end{gather}
We note that the likelihood for the Poisson distribution factorizes as in the following:
\begin{align}
	\mathcal{L}(\lambda|\textbf{x})=p(\textbf{x}|\lambda)=\prod_{i=1}^n \frac{\lambda^{x_i}e^{-\lambda}}{x_i!}= \left( \prod_{i=1}^n x_i! \right)^{-1} \times \lambda^{\sum_{i=1}^n x_i} e^{-n\lambda}. 
	\label{eq:poissonLik}
\end{align}
From Eq. \ref{eq:poissonLik} we can note that the likelihood term that involves $\lambda$ and are is not fixed by the sample $\textbf{x}$ is $\lambda^{\sum_{i=1}^n x_i} e^{-n\lambda}$. Thus the posterior distribution for $\lambda$ has the following form:
\begin{align}
	p(\lambda|\textbf{x})\propto p(\lambda) p(\textbf{x}|\lambda) \propto p(\lambda)\lambda^{\sum_{i=1}^n x_i} e^{-n\lambda}
\end{align}
Considering the prior distribution that we have specified:
\begin{align}
	p(\lambda|\textbf{x})&\propto \frac{\beta^{\alpha}}{\Gamma(\alpha)}\lambda^{\alpha-1}e^{-\beta \lambda} \lambda^{\sum_{i=1}^n x_i} e^{-n\lambda} \\
	&\propto \frac{\beta^{\alpha}}{\Gamma(\alpha)} \lambda^{\alpha + \sum_{i=1}^n x_i -1} e^{-(\beta+n)\lambda},
\end{align}
we can note that:
\begin{align}
p(\lambda|\textbf{x}) =c(\textbf{x},\alpha,\beta)^{-1} \lambda^{\alpha + \sum_{i=1}^n x_i -1} e^{-(\beta+n)\lambda},
\end{align}
where $c(\textbf{x},\alpha,\beta)$ is a normalizing constant that depends only on the prior distribution parameters and the observed sample. Thus it is evident that the posterior distribution for $\lambda$ is also a Gamma distribution, specifically $p(\lambda|\textbf{x})\sim Gamma(\alpha+ \sum_{i=1}^n x_i, \beta+n)$. Fig.~\ref{fig:pois} shows the pdf of both the prior and posterior probability distributions for the Poisson rate parameter $\lambda$.

\begin{figure}[ht]
	\resizebox{\textwidth}{!}{\includegraphics*{./Plots/Poisson.png}}
	\caption[Example Bayesian estimation of a Poisson rate parameter]{\DoubleSpacing Example Bayesian estimation of a Poisson rate parameter. The plot shows the probability density function of the prior on posterior distributions for the Poisson rate parameter $\lambda$. Dashed blue vertical line shows the mode of the posterior distribution. Also shown as a dashed black vertical line is the maximum likelihood estimator determined from the sample $\textbf{x}$. \label{fig:pois} }
\end{figure}

As can be observed, the center (mode or mean) of the posterior probability distribution for $\lambda$ lies between the maximum likelihood estimator (MLE) for $\lambda$ and the prior distribution. In fact, while the MLE for $\lambda$ is $\hat{\lambda}_{MLE}=105.7143$, the mode of the posterior distribution of $\lambda$ is:
\begin{align*}
	\tilde{\lambda} = \frac{\alpha+ \sum_{i=1}^n x_i -1}{\beta+n} =\frac{1000+105.7143}{10+n} = 102.2941.
\end{align*}

In addition to point-wise summaries (such as the mode of the posterior distribution), interval summaries can be provided utilizing the posterior distribution of a parameter. In this case, a Bayesian credible interval \cite{gelman2004} can be analytically computed using a quantile function as the analytical form of the posterior distribution is known. Specifically, a two-sided 95\% Bayesian credible interval (CI) can be determined by finding the unique solution to $(l,u)$, such that $F(l|\alpha+ \sum_{i=1}^n x_i -1,\beta+n)=0.025$ and $F(u|\alpha+ \sum_{i=1}^n x_i -1,\beta+n)=.975$ where $F(x)$ is the cumulative distribution function for the Gamma distribution. In the current example, this yields a 95\% CI of $(97.600, 107.218)$.
 
\section{Specification of prior distribution}
Given that the posterior distribution of parameters depends on both the data likelihood and the prior distribution, the specification of prior distribution is of critical interest in Bayesian inference. While a general distinction is made between ``informative'' priors and ``non-informative'' priors \cite{gelman2006}, ``weekly informative'' priors represent an in-between. Non-informative priors are prior distributions that are vague, flat, diffuse \cite{gelman2004}, and do not incorporate external information into posterior probability distributions. Weekly informative priors generally incorporate some other goal, such as encouraging regularization without introducing specific \emph{a priori} knowledge about the prior likelihood of parameter values. In contrast, informative priors do incorporate external information (outside of the empirical data) \cite{gelman2004}. Informative priors may be generated from previous experimentation, often by the use of conjugate distributions \cite{hoff2009}. 

\begin{definition}{Conjugate probability distribution}
	Let $\mathcal{F}$ be a class of probability distributions from which $x$ may be sampled with parameter $\theta$, that is $f(x|\theta)$ and let $\mathcal{P}$ be a class of prior distributions for $\theta$. $\mathcal{P}$ is said to be cojugate to $\mathcal{F}$ if $p(\theta|x)\in \mathcal{P} \; \forall f(\cdot | \theta) \in \mathcal{F} \text{ and } p(\cdot) \in \mathcal{P}$.
\end{definition}

Following these definitions, we return to our discussion of the counting mass detector example. This example utilized an informative conjugate prior distribution for the Poisson rate parameter. Given the form of the data-augmented posterior distribution, $p(\lambda|\textbf{x})\sim Gamma(\alpha+ \sum_{i=1}^n x_i, \beta+n)$,  the posterior Gamma distribution parameters can be noted as $\alpha'=\alpha+ \sum_{i=1}^n x_i$ and $\beta'=\beta+n$, where $\alpha$ and $\beta$ are the prior parameters. This provides a natural way to encapsulate prior empirical evidence. Noting that prior expectation for $\lambda$ is $\EE(\lambda)=\frac{\alpha}{\beta}$, while the posterior expectation conditional expectation is:
\begin{align}
	\EE(\lambda|\textbf{x})=\frac{\alpha+ \sum_{i=1}^n x_i}{\beta+n} =\frac{\alpha}{\beta+n}+\frac{\sum_{i=1}^n x_i}{\beta+n},
\end{align} it can be seen that if the sum of counts observed in $\beta$ previous experiments (where $\beta$ is a natural number) was $\alpha$, then the conditional expectation of $\lambda$ is a weighted average of the prior observed counts and the current observed counts (weighted by the respective number of observations). 

\section{MCMC methods for inference}
Many posterior probability distributions do not have an expression in analytical form, or it may be challenging to determine such a form. In such cases, Markov chain Monte Carlo (MCMC) methods may be utilized for simulating the posterior distribution. The fundamental principal of Monte Carlo methods is that if an integral $I=\int_{\Omega} f(\textbf{x}) d\textbf{x}$ cannot be obtained analytically, then the law of large numbers justifies generating a random sample $\{\textbf{X}_i \}_{i=1}^n$ from a distribution $p(x)$ over $\Omega$ and using it for approximation \cite{gelman2004}, that is:
\begin{align}
I \approx  \frac{1}{n} \sum_{i=1}^n g(\textbf{X}_i),
\end{align}
where $g(x)= f(x) / p(x)$. Two commonly utilized MCMC approaches are Metropolis-Hastings type algorithms which rely on formulating a proposal density for accepting or rejecting random moves according to the target distribution and Gibbs sampling which draws from conditional distributions.

\subsection{Gibbs Sampling}
In addition to not having an analytical form of the joint posterior distribution of model parameters (which justifies the use of posterior simulation) it may be the case that it is straightforward to sample from conditional distributions of parameters but not for sampling from the joint distribution. In such cases, Gibbs sampling may be employed \cite{gelman2004,hoff2009}. The idea of Gibbs sampling is straightforward. First, parameters estimates are initialized to some initial values. Next, one parameter is fixed as the target parameter. A sample is then drawn from the conditional distribution of this target parameter (conditioned on both the non-target parameter estimates and the observed sample). Gibbs sampling can be extended to using target sets of parameters without loss of generality. A more precise definition of a Gibbs sampler is as follows and pseudocode is presented as Alg.~\ref{alg:gibbs}. Let $\boldsymbol{\theta}=\{\theta_1, \theta_2, \hdots,  \theta_p \}$ be a vector of parameters and let $\boldsymbol{\theta}_j$ be a vector defined by a subset of the parameters with the subsets indexed by $j=1, 2, \hdots, J$. Let $\tilde{\boldsymbol{\theta}}_j^{(t)}$ denote the estimate of the subset of parameters at time $t$. 

\begin{algorithm}
	\caption{Gibbs sampler
		\label{alg:gibbs}}
\begin{algorithmic}[1]
	\STATE Set $\boldsymbol{\theta}^{(0)}$ to some initial values
	\WHILE{$t\leq t_{max}$ or convergence criteria not met}
		\STATE Set $\tilde{\boldsymbol{\theta}} \leftarrow \tilde{\boldsymbol{\theta}}^{(t-1)}$
		\FOR{$j=1$ to $J$}
			\STATE Sample $\tilde{\boldsymbol{\theta}}_j^{*}\sim p \left(\boldsymbol{\theta}_j|\tilde{\boldsymbol{\theta}}_{-j}, \textbf{x} \right)$
			\STATE Update $\tilde{\boldsymbol{\theta}}_j \leftarrow \tilde{\boldsymbol{\theta}}_j^{*}$
		\ENDFOR
		\STATE Set $\tilde{\boldsymbol{\theta}}^{(t)} \leftarrow \tilde{\boldsymbol{\theta}}$ and return $\tilde{\boldsymbol{\theta}}^{(t)} $
		\STATE $t \leftarrow t+1$
	\ENDWHILE
\end{algorithmic}
\end{algorithm} 

\subsection{Example application of Gibb's sampling}
Continuing with the example of a mass spectrometer with a counting detector, we illustrate the use of Gibb's sampling for simulating the posterior distribution of a set of model parameters. In the previous example, we assumed that there was only one population rate parameter. In the current example, we consider a case in which, over a fixed m/z window, counts may be observed for three types of ions within the same m/z window as opposed to one. To the observer, it is not clear which ions the counts are originating from. We would expect, however that the population rate parameters are different for each ion. This can be formulated using the following model \cite{diebolt1994,everitt2004}. First we note the pmf for individual observations $x_i$:
\begin{align}
	f(x_i|\boldsymbol{\lambda},\textbf{p})=\sum_{j=1}^3 p_j \frac{e^{-\lambda_j} \lambda_j^{x_i}}{x_i!}
\end{align}
where $j$ is the indicator for the source ion, each $x_i$ is a random variable representing the counts, $\lambda_j$ represents the population rate parameter for each $j$ source ion, and $p_j$ represents the probability that a count observation belongs to the $j$ source ion. For each observation $i$ we then introduce a vector of latent variable for indicating which source ion the observation originated from: $\textbf{z}_i=(z_{i1}, z_{i2},z_{i3})$ where:
\begin{align}
	z_{ij}= \left\{ \begin{array}{c l}
	1 & \text{if the counts $x_i$ originate from source ion $j$}   \\ 
	0 & \text{else} 
	\end{array} \right. .
\end{align}
The individual $z_{ij}$ are then Bernoulli distributed with probability $p_j$ and the individual vectors $\textbf{z}_i$ are distributed as $\textbf{z}_{i} \sim Multi(1,p_1,p_2,p_3)$ with $\sum_{j=1}^3 p_j =1$. Given the observations are exchangeable, we then have the indicator vector likelihood :
\begin{align}
	 f(\textbf{z}|\textbf{p})= \prod_{i=1}^n f(\textbf{z}_i|\textbf{p}) = \prod_{i=1}^n \prod_{j=1}^3 p_j^{z_{ij}}.
\end{align}
We then can specify a (conjugate) prior distribution for $\textbf{p}$, that is $f(\textbf{p})\sim Dirichlet(\boldsymbol{\alpha})$. We choose the non-informative prior $\alpha_j=1 \; \forall j$. We place a Gamma distribution with shape and scale priors $r$ and $s$ over the rate parameters, that is: $\lambda_j \sim Gamma(r,s)$. and we assume that the rate parameters are independent (of each other prior parameter). By application of Bayes' rule and the chain rule, we can determine the posterior distribution of the parameters:
\begin{align}
\label{eq:prop}
\begin{split}
f(\textbf{z},\textbf{p},\boldsymbol{\lambda}|\textbf{x}) &\propto f(\textbf{x}|\textbf{z},\textbf{p},\boldsymbol{\lambda}) f(\textbf{z},\textbf{p},\boldsymbol{\lambda}) \\
&\propto f(\textbf{x}|\textbf{z},\textbf{p},\boldsymbol{\lambda})f(\textbf{z},\textbf{p}) f(\boldsymbol{\lambda}) \\
&\propto f(\textbf{x}|\textbf{z},\textbf{p},\boldsymbol{\lambda})f(\textbf{z}|\textbf{p}) f(\textbf{p}) f(\boldsymbol{\lambda}).
\end{split}
\end{align}
Denoting the gamma pdf's for each $\lambda_j$ as $f_{\lambda_j}(\lambda_j)$, and likewise the pdf for the Dirichlet prior as $f_{\textbf{p}}(\textbf{p})$, we can then enumerate Eq. \ref{eq:prop} as:
\begin{align}
	f(\textbf{z},\textbf{p},\boldsymbol{\lambda}|\textbf{x}) &\propto \prod_{i=1}^{n} \prod_{j=1}^3 \left(\frac{e^{-\lambda_j}\lambda_j^{x_i}}{x_i!}\right)^{z_{ij}} \times \prod_{i=1}^{n} \prod_{j=1}^3 (p_j)^{z_{ij}} \times \prod_{j=1}^3 f_{\lambda_j}(\lambda_j) \times f_{\textbf{p}}(\textbf{p})
\end{align}
From this, conditional distributions for each parameter are apparent. For the vector of Bernoulli parameters:
\begin{align}
\begin{split}
	f(\textbf{p}|\textbf{z},\boldsymbol{\lambda},\textbf{x}) &\propto \prod_{i=1}^n \prod_{j=1}^3 (p_j)^{z_{ij}} f_{\textbf{p}}(\textbf{p}) \\
	&\propto \prod_{i=1}^n \prod_{j=1}^3 (p_j)^{z_{ij}} \\
	&\propto \prod_{j=1}^3 p_j^{\sum_{i=1}^n z_{ij}} \\
	&\propto Dirichlet(\boldsymbol{\alpha}'),
\end{split}
\end{align}
where $\boldsymbol{\alpha}'=(\alpha_{1}',\alpha_{2}',\hdots, \alpha_{k}')$ and $\alpha_{j}=1+\sum_{i=1}^n z_{i1}$.

For the rate parameters conditional on the other parameters (using the first as an example), we have:
\begin{align}
\begin{split}
	f(\lambda_1|\boldsymbol{\lambda}^{(-1)},\textbf{p},\textbf{z},\textbf{x})&\propto \prod_{i=1}^{n} \prod_{j=1}^3 \left(\frac{e^{-\lambda_j}\lambda_j^{x_i}}{x_i!}\right)^{z_{ij}} f_{\lambda_1}(\lambda_1) \\
	&\propto \prod_{i=1}^{n} \left(\frac{e^{-\lambda_j}\lambda_j^{x_i}}{x_i!}\right)^{z_{ij}} \frac{s_1^{r_1}}{\Gamma(r_1)} \lambda_1^{r_1-1} e^{-s_1 \lambda_1}\\
	&\propto \prod_{i=1}^{n} \left( e^{-\lambda_j}\lambda_j^{x_i} \right)^{z_{ij}} \left( \lambda_1^{r_1-1} e^{-s_1 \lambda_1} \right) \\
	&= e^{-\lambda_1 (\sum_{i=1}^n z_{i1}+s_1)}+\lambda_1^{\sum_{i=1}^n x_i z_{i1}+r_1-1}.
\end{split}
\end{align}
It can thus be seen that the conditional distribution for $\lambda_1$ (conditioned on the remaining parameters and the data) is conjugate and is proportional to the following distribution:
\begin{align}
		f(\lambda_1|\boldsymbol{\lambda}^{(-1)},\textbf{p},\textbf{z},\textbf{x})&\propto Gamma \left( r_1+\sum_{i=1}^n x_i z_{i1}, s_1+\sum_{i=1}^n z_{i1} \right). 
\end{align}

Finally, for the latent indicator variables:
\begin{align}
	\begin{split}
f(z_{ij}=1|x_i,\boldsymbol{\lambda},\textbf{p})&=
\frac{f(x_i|\boldsymbol{\lambda},\textbf{p},z_{ij}=1)f(z_{ij}=1|\boldsymbol{\lambda},\textbf{p},x_i)}{\sum_{j=1}^3 f(x_i|\boldsymbol{\lambda},\textbf{p},z_{ij}=1)f(z_{ij}=1|\boldsymbol{\lambda},\textbf{p},x_i)} \\
&=\frac{f(x_i|\boldsymbol{\lambda},z_{ij}=1)f(z_{ij}=1|\boldsymbol{\lambda},\textbf{p})}{\sum_{j=1}^3 f(x_i|\boldsymbol{\lambda},z_{ij}=1)f(z_{ij}=1|\boldsymbol{\lambda},\textbf{p})} \\
&=\frac{f(x_i|\lambda_j)p_j}{\sum_{j=1}^3 f(x_i|\lambda_j)p_j} \\
&= \frac{f(x_i|\lambda_j)p_j}{ f(x_i)}.
	\end{split}
\end{align}
Thus it can be seen that the conditional distribution for each $z_i^{(-i)}$ is:
\begin{align}
	z_i^{(-i)} |(\boldsymbol{\theta},\textbf{p},\textbf{z},\textbf{x} )\sim Multi \left(1,\frac{f(x_i|\lambda_1)p_1}{ f(x_i)},\hdots , \frac{f(x_i|\lambda_3)p_3}{ f(x_i)} \right)
\end{align}
From these conditional distributions, a Gibbs sampler can be formulated.

\end{DoubleSpace*}