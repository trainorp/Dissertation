\label{prelimBayesian}
\begin{DoubleSpace*}
\section{Bayes rule}

The central foundation of Bayesian statistics is Bayes rule \cite{gelman2004}. 
\begin{theorem} If $A$ and $B$ are events with $\PP(B)>0$, then:
	\begin{gather}
		\PP(A|B) = \frac{\PP(B|A)\PP(A)}{\PP(B)}
	\end{gather}.	
\end{theorem}

Further, Bayes rule can be extended to any arbitrary partitioning of a sample space \cite{casella2002}.
\begin{theorem}
	Let $\mathcal{P}=\{A_i: i=1,2, \hdots, N\}$ represents an arbitrary partitioning of a sample space $S$, then for a specific $A_i$:
	\begin{gather}
			 \PP(A_i|B)=\frac{\PP(B|A_i)P(A_i)}{\sum_{A_j\in \mathcal{P}}\PP(B|A_j)P(A_j)}
	\end{gather}.
\end{theorem}

\section{Bayesian inference regarding a parameter}
Let $\theta$ (or $\boldsymbol{\theta}$ in the multivariate case) represent a parameter of a probability mass function (pmf) or probability distribution function (pdf)  $f(x|\theta)$ of a random variable $X$. Without loss of generality, we discuss the univariate case of one random variable $X$ and one population parameter $\theta$, although multivariate generalization is straightforward. The objective of statistical inference regarding $\theta$ is to utilize a random sample $X_1, X_2, \hdots X_n$ from a population with pmf/pdf $f(x|\theta)$ to determine likely values of $\theta$ \cite{casella2002}. Denoting a fixed sample as $\textbf{x}=\{x_1, x_2,\hdots, x_n \}$, we note that $\textbf{x}$ is a realization from the sample space $\mathcal{X}$ of all possible samples from the population. In Bayesian inference, the population parameter $\theta$ is regarded as unknown \cite{hoff2009}. It is assumed that the uncertainty regarding the true population value of $\theta$, can be represented by the prior probability distribution $p(\theta)$. Since the true population value is unknown, a set of possible values for $\theta$, that is the parameter space $\Theta$ comprises the support of $p(\theta)$. In Bayesian analysis, a sampling model describing the probability of observing realization $x$ of the random variable $X$ conditioned on a fixed value of the parameter $\theta$, that is $p(x|\theta)$ must also be specified. When considering a fixed sample, this term is also referred to as the  likelihood \cite{casella2002}. 
\begin{theorem}
	 If $X_1, X_2, \hdots, X_n$ are independent and identically distributed (iid) random variables, then $f(x_1,x_2,\hdots, x_n|\theta)=\prod_{i=1}^n f(x_i|\theta)$.
\end{theorem}
More specifically, we refer to the joint distribution function of a sample $x$, conditional on the value of the parameter $\theta$ as the likelihood of theta given the sample, or $\mathcal{L}(\theta|\textbf{x})$. Given a prior distribution for $\theta\in \Theta$, a random sample from the population $\textbf{x}$,and a sampling model or likelihood, the posterior distribution for $\theta$ can then be determined utilizing Bayes rule, as below\cite{hoff2009}:
\begin{gather}
	p(\theta|\textbf{x})=\frac{p(\textbf{x}|\theta)p(\theta)}{\int_{\tilde{\theta} \in \Theta} p(\textbf{x}|\tilde{\theta})p(\tilde{\theta})d\tilde{\theta}}=\frac{p(\textbf{x}|\theta)p(\theta)}{p(\textbf{x})}.
\end{gather}
The denominator term is referred to as the marginal likelihood. As the marginal likelihood involves only a fixed sample, the denominator is a constant, which justifies the following relation involving the posterior distribution of $\theta$:
\begin{gather}
	p(\theta|\textbf{x}) \propto p(\textbf{x}|\theta)p(\theta). 
\end{gather}
From this relation, it can be noted that the posterior distribution for $\theta$, after observing a random sample $\textbf{x}$ is proportional to the likelihood times the prior.

\section{Example Bayesian inference regarding a parameter}
Consider a mass spectrometer with a counting detector that records the counts per second over a fixed $m/z$ window. The number of counts recorded by the detector over a fixed time interval may be a Poisson process. Let $X$ be the random variable representing the number of counts recorded by the detector over a one second time interval, with an average number of counts per second (population parameter) of $\lambda$. We then say that $X\sim Pois(\lambda)$, and note that the probability mass function for $X$ is:
\begin{gather}
	f(x|\lambda)=\frac{\lambda^x e^{-\lambda}}{x!}, \quad x \in \mathbb{N}, \quad \lambda \in \left[0, \infty \right)
\end{gather}
 Assume a sample has been observed with the following counts: $\{107, 103, 110, 99, 108, 108, 105\}$ and that we wish to determine plausible values of the rate parameter $\lambda$. In order to determine plausible values, we will conduct Bayesian inference over $\lambda$. Assume that we have collected ten prior count observations and that the sum of counts from these observations was 1,000. We may then choose a prior distribution for $\lambda$ of $Gamma(\alpha, \beta)$ with parameters $\alpha=1000$ and $\beta=10$. We note the following form of the Gamma distribution pdf: 
\begin{gather}
	f(\lambda|\alpha,\beta) =\frac{\beta^{\alpha}}{\Gamma(\alpha)}\lambda^{\alpha-1}e^{-\beta \lambda}, \quad \lambda \in \left[0, \infty \right), \quad \alpha,\beta >0.
\end{gather}
We note that the likelihood for the Poisson distribution factorizes as in the following:
\begin{align}
	\mathcal{L}(\lambda|\textbf{x})=p(\textbf{x}|\lambda)=\prod_{i=1}^n \frac{\lambda^{x_i}e^{-\lambda}}{x_i!}= \left( \prod_{i=1}^n x_i! \right)^{-1} \times \lambda^{\sum_{i=1}^n x_i} e^{-n\lambda}. 
	\label{eq:poissonLik}
\end{align}
From Eq. \ref{eq:poissonLik} we can note that the likelihood term that involves $\lambda$ and are is not fixed by the sample $\textbf{x}$ is $\lambda^{\sum_{i=1}^n x_i} e^{-n\lambda}$. Thus the posterior distribution for $\lambda$ has the following form:
\begin{align}
	p(\lambda|\textbf{x})\propto p(\lambda) p(\textbf{x}|\lambda) \propto p(\lambda)\lambda^{\sum_{i=1}^n x_i} e^{-n\lambda}
\end{align}
Considering the prior distribution that we have specified:
\begin{align}
	p(\lambda|\textbf{x})&\propto \frac{\beta^{\alpha}}{\Gamma(\alpha)}\lambda^{\alpha-1}e^{-\beta \lambda} \lambda^{\sum_{i=1}^n x_i} e^{-n\lambda} \\
	&\propto \frac{\beta^{\alpha}}{\Gamma(\alpha)} \lambda^{\alpha + \sum_{i=1}^n x_i -1} e^{-(\beta+n)\lambda},
\end{align}
we can note that:
\begin{align}
p(\lambda|\textbf{x}) =c(\textbf{x},\alpha,\beta)^{-1} \lambda^{\alpha + \sum_{i=1}^n x_i -1} e^{-(\beta+n)\lambda},
\end{align}
where $c(\textbf{x},\alpha,\beta)$ is a normalizing constant that depends only on the prior distribution parameters and the observed sample. Thus it is evident that the posterior distribution for $\lambda$ is also a Gamma distribution, specifically $p(\lambda|\textbf{x})\sim Gamma(\alpha+ \sum_{i=1}^n x_i, \beta+n)$. Fig.~\ref{fig:pois} shows the pdf of both the prior and posterior probability distributions for the Poisson rate parameter $\lambda$.

\begin{figure}[ht]
	\resizebox{\textwidth}{!}{\includegraphics*{./Plots/Poisson.png}}
	\caption[Example Bayesian estimation of a Poisson rate parameter]{\DoubleSpacing Example Bayesian estimation of a Poisson rate parameter. The plot shows the probability density function of the prior on posterior distributions for the Poisson rate parameter $\lambda$. Dashed blue vertical line shows the mode of the posterior distribution. Also shown as a dashed black vertical line is the maximum likelihood estimator determined from the sample $\textbf{x}$. \label{fig:pois} }
\end{figure}

As can be observed, the center (mode or mean) of the posterior probability distribution for $\lambda$ lies between the maximum likelihood estimator (MLE) for $\lambda$ and the prior distribution. In fact, while the MLE for $\lambda$ is $\hat{\lambda}_{MLE}=105.7143$, the mode of the posterior distribution of $\lambda$ is:
\begin{align*}
	\tilde{\lambda} = \frac{\alpha+ \sum_{i=1}^n x_i -1}{\beta+n} =\frac{1000+105.7143}{10+n} = 102.2941.
\end{align*}

In addition to point-wise summaries (such as the mode of the posterior distribution), interval summaries can be provided utilizing the posterior distribution of a parameter. In this case, a Bayesian credible interval \cite{gelman2004} can be analytically computed using a quantile function as the analytical form of the posterior distribution is known. Specifically, a two-sided 95\% Bayesian credible interval (CI) can be determined by finding the unique solution to $(l,u)$, such that $F(l|\alpha+ \sum_{i=1}^n x_i -1,\beta+n)=0.025$ and $F(u|\alpha+ \sum_{i=1}^n x_i -1,\beta+n)=.975$ where $F(x)$ is the cumulative distribution function for the Gamma distribution. In the current example, this yields a 95\% CI of $(97.600, 107.218)$.
 
\section{Specification of prior distribution}
Conjugate distributions \\

Given the form of the data-augmented posterior distribution, $p(\lambda|\textbf{x})\sim Gamma(\alpha+ \sum_{i=1}^n x_i, \beta+n)$,  the posterior Gamma distribution parameters can be noted as $\alpha'=\alpha+ \sum_{i=1}^n x_i$ and $\beta'=\beta+n$, where $\alpha$ and $\beta$ are the prior parameters. This provides a natural way to encapsulate prior empirical evidence. Noting that prior expectation for $\lambda$ is $\EE(\lambda)=\frac{\alpha}{\beta}$, while the posterior expectation conditional expectation is:
\begin{align}
	\EE(\lambda|\textbf{x})=\frac{\alpha+ \sum_{i=1}^n x_i}{\beta+n} =\frac{\alpha}{\beta+n}+\frac{\sum_{i=1}^n x_i}{\beta+n},
\end{align} it can be seen that if the sum of counts observed in $\beta$ previous experiments (where $\beta$ is a natural number) was $\alpha$, then the conditional expectation of $\lambda$ is a weighted average of the prior observed counts and the current observed counts (weighted by the respective number of observations). 

\section{MCMC methods for inference}

\subsection{Gibbs Sampling}
In addition to not having an analytical form of the joint posterior distribution of model parameters (which justifies the use of posterior simulation) it may be the case that it is straightforward to sample from conditional distributions of parameters but not for sampling from the joint distribution. In such cases, Gibbs sampling may be employed \cite{gelman2004,hoff2009}. The idea of Gibbs sampling is straightforward. First, parameters estimates are initialized to some initial values. Next, one parameter is fixed as the target parameter. A sample is then drawn from the conditional distribution of this target parameter (conditioned on both the non-target parameter estimates and the observed sample). Gibbs sampling can be extended to using target sets of parameters without loss of generality. A more precise definition of a Gibbs sampler is as follows and pseudocode is presented as Alg.~\ref{alg:gibbs}. Let $\boldsymbol{\theta}=\{\theta_1, \theta_2, \hdots,  \theta_p \}$ be a vector of parameters and let $\boldsymbol{\theta}_j$ be a vector defined by a subset of the parameters with the subsets indexed by $j=1, 2, \hdots, J$. Let $\tilde{\boldsymbol{\theta}}_j^{(t)}$ denote the estimate of the subset of parameters at time $t$. 

\begin{algorithm}
	\caption{Gibbs sampler
		\label{alg:gibbs}}
\begin{algorithmic}[1]
	\STATE Set $\boldsymbol{\theta}^{(0)}$ to some initial values
	\WHILE{$t\leq t_{max}$ or convergence criteria not met}
		\STATE Set $\tilde{\boldsymbol{\theta}} \leftarrow \tilde{\boldsymbol{\theta}}^{(t-1)}$
		\FOR{$j=1$ to $J$}
			\STATE Sample $\tilde{\boldsymbol{\theta}}_j^{*}\sim p \left(\boldsymbol{\theta}_j|\tilde{\boldsymbol{\theta}}_{-j}, \textbf{x} \right)$
			\STATE Update $\tilde{\boldsymbol{\theta}}_j \leftarrow \tilde{\boldsymbol{\theta}}_j^{*}$
		\ENDFOR
		\STATE Set $\tilde{\boldsymbol{\theta}}^{(t)} \leftarrow \tilde{\boldsymbol{\theta}}$ and return $\tilde{\boldsymbol{\theta}}^{(t)} $
		\STATE $t \leftarrow t+1$
	\ENDWHILE
\end{algorithmic}
\end{algorithm} 

\subsection{Example application of Gibb's sampling}
Continuing with the example of a mass spectrometer with a counting detector, we illustrate the use of Gibb's sampling for simulating the posterior distribution of a set of model parameters. In the previous example, we assumed that there was only one population rate parameter. In the current example, we consider the fact that over a fixed m/z window, counts may be observed or not observed based on whether an ion is present. If the ion is not present, we expect to observe a count of zero; if the ion is present we expect the counts to follow a Poisson distribution. This can be formulated as the following hierarchical model:
\begin{align}
	Z|p,\lambda \sim Bernoulli(p) \\
	X|p,\lambda,z \sim F(x|z,\lambda)
\end{align}
where
\begin{align}
f(x; z, \lambda) =\frac{\lambda^x e^{-\lambda}}{\lambda!} I_{\{z=1\}}(z)+ I_{\{z=0\}}
\end{align}
\end{DoubleSpace*}